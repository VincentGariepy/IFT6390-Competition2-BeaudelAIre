{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JwsSBUhGtP2A",
        "MkdjZxCE5rbP",
        "kCm4dngA3RDR",
        "hLxk16iar89Q",
        "u2hhjD2yttLo",
        "5xlp_RRKyugZ",
        "7wrDVxNWpiua",
        "v3Ywl8BXHka1",
        "__3PxTxExL54",
        "0Oop6Ym6WGZ5",
        "BV-PWCC64BGj"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentGariepy/IFT6390-Competition2-BeaudelAIre/blob/main/NB_SVM_LIME/Comp%C3%A9tition_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compétition 2\n",
        "Anshita Saxena, Denis Lemarchand, Vincent Gariépy"
      ],
      "metadata": {
        "id": "cpd6o6snv4f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "f4Pkv3flyNNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoLvWqWEvy82"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upload Kaggle File from Google Drive"
      ],
      "metadata": {
        "id": "JcyTy6CXyQ6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install PyDrive"
      ],
      "metadata": {
        "id": "JwsSBUhGtP2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "metadata": {
        "id": "2-uRO-bywXfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read files and put it in Panda"
      ],
      "metadata": {
        "id": "1GuPPqzLtUW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#This notebook should access to Google Drive\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "wNF-CChvyt9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data\n",
        "# https://drive.google.com/file/d/17cD6hFS_AfKxtfjKxfeNUkC44jfSYkGj/view?usp=sharing\n",
        "# train result\n",
        "# https://drive.google.com/file/d/1iEpwQ3B4d4gIUl3ynq_p74U1QKZTURem/view?usp=sharing\n",
        "# test data\n",
        "# https://drive.google.com/file/d/1QbhqakgLpBXWr5sMFSe-BQpFw_X2XZZR/view?usp=sharing\n",
        "\n",
        "#download Kaggle Files from Google Drive to store it in Colab Session\n",
        "downloaded = drive.CreateFile({'id':\"17cD6hFS_AfKxtfjKxfeNUkC44jfSYkGj\"})   \n",
        "downloaded.GetContentFile('train.csv') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1iEpwQ3B4d4gIUl3ynq_p74U1QKZTURem\"})   \n",
        "downloaded.GetContentFile('train_result.csv')    \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1QbhqakgLpBXWr5sMFSe-BQpFw_X2XZZR\"})   \n",
        "downloaded.GetContentFile('test.csv') "
      ],
      "metadata": {
        "id": "nwyJrWUby2G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "df_train = pd.read_csv('train.csv')\n",
        "#\n",
        "df_train_result = pd.read_csv('train_result.csv')\n",
        "#\n",
        "df_test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "RF2vr98u1J8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate submission file"
      ],
      "metadata": {
        "id": "MkdjZxCE5rbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make submission file from numpy data\n",
        "#The value of class should be 0, 1 or 2 with 0 being negative, 1 being neutral and 2 being positive class.\n",
        "def PrepareKaggleFile(test_inputs, test_predictions, file='tests_label.csv'):\n",
        "    output_data_for_kaggle = np.zeros((len(test_inputs),2))\n",
        "    for i in range(len(test_inputs)):\n",
        "      output_data_for_kaggle[i,0] = i\n",
        "      output_data_for_kaggle[i,1] = test_predictions[i]\n",
        "\n",
        "    output_data_for_kaggle = output_data_for_kaggle.astype(int)\n",
        "\n",
        "    print(output_data_for_kaggle)\n",
        "\n",
        "    df = pd.DataFrame(data=output_data_for_kaggle,columns=['id','target'])\n",
        "    df.to_csv(file,index=False)"
      ],
      "metadata": {
        "id": "wvp8zXNM5u9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing\n",
        "\n",
        "References:\n",
        "*   Data preprocessing techniques: https://www.mdpi.com/2078-2489/12/9/374/htm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kCm4dngA3RDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_neutral_mode = True\n",
        "preprocessing_cleanning_mode = True\n",
        "\n",
        "#lowercase_mode = False #0.7775705785664164\n",
        "lowercase_mode = True #0.7799928868724347 --> baseline\n",
        "\n",
        "remove_video_mode = False #True: 0.7799928868724347 --> no impact\n",
        "remove_html_mode = False #True: 0.7796852921669086 --> no improvement\n",
        "clean_stop_word_mode = False #True: 0.7701402439610509 --> no improvement\n",
        "remove_punctuation_mode = True #True: 0.7800890102179117 --> improvement\n",
        "remove_hashmarks_mode = False #True: 0.7795891688214317 --> no improvement\n",
        "remove_number_mode = False #True: 0.7797718031778378 --> no improvement\n",
        "remove_username_mode = False #True: 0.7789643670758317 --> no improvement\n",
        "remove_url_mode = True #True: 0.7824055828439053 --> improvement\n",
        "stemming_mode = False #True: 0.775244393605875 --> no improvement"
      ],
      "metadata": {
        "id": "OtGXwW59z9r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing dependencies\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re                                  \n",
        "import string                             \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "1pKeJayPA7wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion from categorical labels to numeric labels\n",
        "df_train_result[df_train_result[\"target\"]=='negative']=0\n",
        "df_train_result[df_train_result[\"target\"]=='neutral']=1\n",
        "df_train_result[df_train_result[\"target\"]=='positive']=2"
      ],
      "metadata": {
        "id": "fkr4qD6VbkB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the train content\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "OkL-VO8Ssbca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the target content\n",
        "df_train_result.head()"
      ],
      "metadata": {
        "id": "uaPiRhSAsnh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation and numbers\n",
        "def remove_punct(text):\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "\"\"\"\n",
        "Snowball Stemmer is also known as the Porter2 stemming algorithm \n",
        "because it is a better version of the Porter Stemmer. It is more aggressive than Porter Stemmer.\n",
        "\"\"\"\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowBallStemmer = SnowballStemmer(\"english\")\n",
        "def stemming(text):\n",
        "    text = [snowBallStemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(text)\n",
        "\n",
        "def Preprocessing_Cleanning(df_data):\n",
        "      # Converting all the upper case to lower case to avoid the distinction between them\n",
        "      if lowercase_mode==True:\n",
        "        df_data['text'] = df_data.text.str.lower()\n",
        "\n",
        "      # Putting the regex for removing the https and www URLs\n",
        "      if remove_url_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "\n",
        "      # Remove the video and links\n",
        "      if remove_video_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
        "\n",
        "      # Remove html reference characters\n",
        "      if remove_html_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "\n",
        "      # Remove usernames\n",
        "      if remove_username_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r'@[^\\s]+', '', x))\n",
        "\n",
        "      # Removing numbers\n",
        "      if remove_number_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "      # Removing hashmarks, non-letter characters\n",
        "      if remove_hashmarks_mode==True:\n",
        "        df_data.text = df_data.text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
        "\n",
        "      # Remove punctuation\n",
        "      if remove_punctuation_mode==True:\n",
        "        df_data['text'] = df_data['text'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "      # Remove stop words\n",
        "      if clean_stop_word_mode==True:\n",
        "          stopword_list = stopwords.words('english')\n",
        "          df_data['text'] = df_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword_list)]))\n",
        "\n",
        "      # stemming\n",
        "      if stemming_mode==True:\n",
        "          df_data['text'] = df_data['text'].apply(lambda x: stemming(x))\n",
        "\n",
        "      return df_data\n"
      ],
      "metadata": {
        "id": "CahohIOm0Szm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of dataset to point them to the different locations in the memory.\n",
        "df_train_clean = df_train.copy()\n",
        "\n",
        "if preprocessing_cleanning_mode==True:\n",
        "    df_train_clean = Preprocessing_Cleanning(df_train_clean)"
      ],
      "metadata": {
        "id": "xEP9VShJtfMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of dataset to point them to the different locations in the memory.\n",
        "df_test_clean = df_test.copy()\n",
        "\n",
        "if preprocessing_cleanning_mode==True:\n",
        "    df_test_clean = Preprocessing_Cleanning(df_test_clean)"
      ],
      "metadata": {
        "id": "dVI3p3W3ttiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_train_clean.to_numpy()\n",
        "train_inputs = train[:]\n",
        "train_results = df_train_result.to_numpy()\n",
        "train_labels = train_results[:,1]\n",
        "test = df_test_clean.to_numpy()\n",
        "test_inputs = test[:]\n",
        "\n",
        "# Smoke Tests\n",
        "list_classes = np.unique(train_labels)\n",
        "print(list_classes)\n",
        "\n",
        "n = np.random.randint(0, len(train_inputs))\n",
        "print(train_inputs[n], train_labels[n])\n",
        "\n",
        "n = np.random.randint(0, len(test_inputs))\n",
        "print(test_inputs[n])\n",
        "\n",
        "dist_train_labels = [np.sum(train_labels==0)/len(train_labels),\n",
        "  np.sum(train_labels==1)/len(train_labels),\n",
        "  np.sum(train_labels==2)/len(train_labels)]\n",
        "\n",
        "print(dist_train_labels)"
      ],
      "metadata": {
        "id": "8nti8x5b22he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reclassification neutral to negative/positive (only 84 samples over 1 million are neutral)\n",
        "# we have now a binary classfication problem to solve\n",
        "train_labels_copy = train_labels.copy()\n",
        "\n",
        "if remove_neutral_mode==True:\n",
        "    print(len(train_labels[train_labels==0]),len(train_labels[train_labels==1]),\n",
        "          len(train_labels[train_labels==2])) #519803:0 (neg) #84:1 (neutral) #520436:2 (pos) \n",
        "    train_labels_copy[train_labels_copy==1] = 2\n",
        "    print(len(train_labels_copy[train_labels_copy==0]),len(train_labels_copy[train_labels_copy==1]),\n",
        "          len(train_labels_copy[train_labels_copy==2]))\n",
        "    dist_train_labels_copy = [np.sum(train_labels_copy==0)/len(train_labels_copy),\n",
        "          np.sum(train_labels_copy==2)/len(train_labels_copy)]\n",
        "    print(dist_train_labels_copy)\n",
        "    dist_train_labels = dist_train_labels_copy"
      ],
      "metadata": {
        "id": "dxXeY2i7tVao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Select Vectorization type"
      ],
      "metadata": {
        "id": "hLxk16iar89Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hashing_vectorizer_mode=True\n",
        "n_features=2**26"
      ],
      "metadata": {
        "id": "unFPqTEEsNsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "if hashing_vectorizer_mode==True:\n",
        "  vectorizer = HashingVectorizer(\n",
        "      decode_error=\"ignore\", n_features=n_features, lowercase=True, binary=True\n",
        "  )\n",
        "else:\n",
        "  vectorizer = TfidfVectorizer(\n",
        "      decode_error=\"ignore\",\n",
        "      lowercase=True, min_df = 0.0, binary=True\n",
        "  )"
      ],
      "metadata": {
        "id": "Zk6MmU_9r_R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create train and validation dataset"
      ],
      "metadata": {
        "id": "u2hhjD2yttLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VPnRq6G7uICN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_text = train_inputs[:,1]\n",
        "train_labels_copy = train_labels_copy.astype(int)\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    train_input_text, train_labels_copy, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "Y22ud0oKty_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM & Naive Bayes\n",
        "Source : https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"
      ],
      "metadata": {
        "id": "IwhOOUEKrhy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Naive_bayes_mode = True\n",
        "batch_mode = False\n",
        "\n",
        "if Naive_bayes_mode==True:\n",
        "  n_features=2**20\n",
        "else:\n",
        "  n_features=2**26"
      ],
      "metadata": {
        "id": "9ZvHSYYY6S56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "import itertools\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "CgA9lVIuryd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source : https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/\n",
        "def create_mini_batches(X, y, batch_size):\n",
        "    mini_batches = []\n",
        "    data = np.hstack((X, y))\n",
        "    np.random.shuffle(data)\n",
        "    n_minibatches = data.shape[0] // batch_size\n",
        "    i = 0\n",
        " \n",
        "    for i in range(n_minibatches + 1):\n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    if data.shape[0] % batch_size != 0:\n",
        "        mini_batch = data[i * batch_size:data.shape[0]]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    return mini_batches"
      ],
      "metadata": {
        "id": "76NrJULW8PM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Naive_bayes_mode == True:\n",
        "  cls = MultinomialNB(alpha=1,class_prior=dist_train_labels) #Naive Bayes\n",
        "  #cls = ComplementNB(alpha=1,class_prior=dist_train_labels_copy)\n",
        "  #cls = BernoulliNB(alpha=1,class_prior=dist_train_labels_copy)\n",
        "else:\n",
        "  class_weight = dict(enumerate(dist_train_labels, 0))\n",
        "  cls = SGDClassifier(max_iter=1, class_weight=class_weight, \n",
        "                    alpha=1e-6, penalty='l2', early_stopping=False, validation_fraction=0.1) #SVM\n",
        "  #cls = svm.SVC(C=1, kernel='linear', max_iter=5)\n",
        "\n",
        "if batch_mode == True:\n",
        "  batch_size = 1000\n",
        "  i = 1\n",
        "  X_test = vectorizer.transform(X_test_text)\n",
        "  mini_batches = create_mini_batches(np.array([X_train_text]).T, np.array([y_train]).T,batch_size)\n",
        "  for mini_batch in mini_batches:\n",
        "      i += 1\n",
        "      X, y = mini_batch\n",
        "      X_train = vectorizer.transform(X[:,0])\n",
        "      cls.partial_fit(X_train, y[:,0].astype(int), classes=np.array([0,1,2]))\n",
        "      if(i%10) == 0:\n",
        "        print(i, X_train.shape, cls.score(X_test, y_test))\n",
        "else:\n",
        "  X_train = vectorizer.fit_transform(X_train_text)\n",
        "  X_test = vectorizer.transform(X_test_text)\n",
        "  cls.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "\n",
        "print(cls.score(X_test, y_test))\n",
        "#best fit SVM : 0.7994482519969625, alpha=0.000001, no processing, n_features=2**26, binary=True, remove_neutral_mode=False\n",
        "#               0.80042 on Kaggle test set\n",
        "#best fit NB : 0.7799928868724347, no processing, n_features=2**20, binary=True, remove_neutral_mode=True"
      ],
      "metadata": {
        "id": "9pSl-HbTvTvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_grid_search = False\n",
        "\n",
        "if mode_grid_search == True:\n",
        "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "    from scipy.stats import uniform, randint\n",
        "\n",
        "    # source : https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn/notebook\n",
        "    def report_best_scores(results, n_top=3):\n",
        "        for i in range(1, n_top + 1):\n",
        "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "            for candidate in candidates:\n",
        "                print(\"Model with rank: {0}\".format(i))\n",
        "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                      results['mean_test_score'][candidate],\n",
        "                      results['std_test_score'][candidate]))\n",
        "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "                print(\"\")\n",
        "\n",
        "    if Naive_bayes_mode == True:\n",
        "      # Find best C and Regularisation for NB\n",
        "\n",
        "      NB_model = MultinomialNB(alpha=1,class_prior=dist_train_labels)\n",
        "\n",
        "      params = {\n",
        "          \"alpha\": [1, 0.1, 1e-3, 1e-5, 1e-6, 1e-7], #uniform(1, 1e-6),\n",
        "      }\n",
        "\n",
        "      search = GridSearchCV(NB_model, param_grid=params,\n",
        "                          verbose=1, n_jobs=1, return_train_score=True)\n",
        "\n",
        "\n",
        "\n",
        "    else: # Find best C and Regularisation for SVM\n",
        "      SVM_model = SGDClassifier(max_iter=100, class_weight=class_weight, \n",
        "                        early_stopping=False, validation_fraction=0.1)\n",
        "\n",
        "      params = {\n",
        "          \"alpha\": [3, 2, 1, 0.1, 1e-3, 1e-5, 1e-7], #uniform(1, 1e-6),\n",
        "          \"penalty\": ['l2', 'l1']\n",
        "      }\n",
        "\n",
        "      search = GridSearchCV(SVM_model, param_grid=params,\n",
        "                          verbose=1, n_jobs=1, return_train_score=True)\n",
        "\n",
        "\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    report_best_scores(search.cv_results_, 1)"
      ],
      "metadata": {
        "id": "GRzLspeeFGBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost"
      ],
      "metadata": {
        "id": "5xlp_RRKyugZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_estimator = None #SGDClassifier(max_iter=100, class_weight=class_weight, \n",
        "                    #alpha=0.0001, penalty='l2', early_stopping=True, validation_fraction=0.1)\n",
        "#svm.SVC(C=1.0, kernel='linear', max_iter=100)#MultinomialNB(alpha=1,class_prior=dist_train_labels_copy)\n",
        "\n",
        "cls = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=20, random_state=0, algorithm='SAMME.R')\n",
        "\n",
        "X_train = vectorizer.transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "print(cls.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "-pADAx5D3TUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost"
      ],
      "metadata": {
        "id": "7wrDVxNWpiua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "cls = xgb.XGBClassifier(n_estimators=2000,max_depth=10, objective=\"binary:logistic\", random_state=42)\n",
        "\n",
        "X_train = vectorizer.transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "print(cls.score(X_test, y_test)) #best: 0.7954591331596705 (1000 estimators, 10 max_depth)"
      ],
      "metadata": {
        "id": "-XgdldPtpndX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RandomForest"
      ],
      "metadata": {
        "id": "v3Ywl8BXHka1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls = RandomForestClassifier(n_estimators=1000,max_depth=20, random_state=0)\n",
        "\n",
        "X_train = vectorizer.transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "print(cls.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "QxgUOtzqHoIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RandomForest from CNAM"
      ],
      "metadata": {
        "id": "__3PxTxExL54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(lowercase=False)\n",
        "train_vectors = vectorizer.fit_transform(X_train_text)\n",
        "test_vectors = vectorizer.transform(X_test_text)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500)\n",
        "rf.fit(train_vectors, y_train)"
      ],
      "metadata": {
        "id": "2zDiFokDxTLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf.score(train_vectors, y_train))\n",
        "print(rf.score(test_vectors, y_test))"
      ],
      "metadata": {
        "id": "sAs-qOxWEvsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = rf.predict(test_vectors)\n",
        "f1_score(y_test, pred, average='binary')"
      ],
      "metadata": {
        "id": "7kt-EcSt4uMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM"
      ],
      "metadata": {
        "id": "YyLkTPq3evFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 128\n",
        "\n",
        "vectorizer = TfidfVectorizer(lowercase=True,stop_words='english',max_features=max_features,strip_accents='ascii')\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train_text)\n",
        "X_test = vectorizer.transform(X_test_text)\n",
        "\n",
        "X_train = X_train.todense()\n",
        "X_test = X_test.todense()"
      ],
      "metadata": {
        "id": "2Dw8ohTheybq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 being negative, 1 being neutral and 2 being positive class.\n",
        "# ---> 0 being negative, 1 being positive\n",
        "y_train_2class = y_train\n",
        "y_train_2class[y_train_2class==2] = 1"
      ],
      "metadata": {
        "id": "p4TwswKPfIn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding"
      ],
      "metadata": {
        "id": "JuDd8v3vfM8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_out = 64\n",
        "embed_dim = 128\n",
        "input_length = X_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, embed_dim,input_length =input_length))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "model.add(LSTM(lstm_out, dropout=0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "XawCKRQPfTI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "seqModel = model.fit(X_train, y_train_2class, epochs = num_epochs, batch_size=batch_size, verbose = 2, \n",
        "          shuffle=False,use_multiprocessing=True,workers=4,validation_split=0.1)"
      ],
      "metadata": {
        "id": "kPQaDtczfXkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# visualizing losses and accuracy\n",
        "train_loss = seqModel.history['loss']\n",
        "val_loss   = seqModel.history['val_loss']\n",
        "train_acc  = seqModel.history['accuracy']\n",
        "val_acc    = seqModel.history['val_accuracy']\n",
        "xc         = range(num_epochs)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xc, train_loss)\n",
        "plt.plot(xc, val_loss)"
      ],
      "metadata": {
        "id": "i3HcY8Fxfbf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 being negative, 1 being neutral and 2 being positive class.\n",
        "# ---> 0 being negative, 1 being positive\n",
        "y_test_2class = y_test\n",
        "y_test_2class[y_test_2class==2] = 1"
      ],
      "metadata": {
        "id": "sg2V1_ETffm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.evaluate(X_test, y_test_2class))"
      ],
      "metadata": {
        "id": "OJA-elZwfjLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'gdrive/MyDrive/perso/MILA/UdeM/data_competition2/neural_network_lstm.h5'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "KFi2n18XfnAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lime"
      ],
      "metadata": {
        "id": "BQ6PdpMRVc4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install Lime"
      ],
      "metadata": {
        "id": "0Oop6Ym6WGZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "ZMpkTUq6WKaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using Lime"
      ],
      "metadata": {
        "id": "zaOSpzVGWLgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lime on lastest model trained : cls\n",
        "import lime\n",
        "\n",
        "from lime import lime_text\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "c = make_pipeline(vectorizer, cls)\n",
        "\n",
        "# 2 times positives because I have reclassed neutral to positive\n",
        "class_names = ['negative','positive', 'positive'] \n",
        "explainer = LimeTextExplainer(class_names=class_names)"
      ],
      "metadata": {
        "id": "TIRd43oDVfjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=1 # 51, is a good prediction (negative)\n",
        "       # 52 is a good prediction (positive), 1 is a bad prediction\n",
        "\n",
        "print('Test document id: %d' % idx)\n",
        "print('Text: %s' % X_test_text[idx])\n",
        "print('Probability(negative) =', c.predict_proba([X_test_text[idx]])[0,0])\n",
        "print('Probability(positive) =', c.predict_proba([X_test_text[idx]])[0,1])\n",
        "print('True class: %s' % class_names[y_test[idx]])\n",
        "print(\"Prediction: %s\" % class_names[cls.predict(X_test[idx])[0]])\n",
        "print(\"-----------------------------------------\")\n",
        "texte = X_test_text[idx]\n",
        "exp = explainer.explain_instance(texte, c.predict_proba, num_features=6,labels=(0,1))\n",
        "fig = exp.as_pyplot_figure(label=int(exp.predict_proba[1]>0.5))"
      ],
      "metadata": {
        "id": "wXk1SmSUXRAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on input text\n",
        "text_to_analyse = input(\"Type and press enter: \")\n",
        "print('Text: %s',text_to_analyse)\n",
        "print('Probability(negative) =', c.predict_proba([text_to_analyse])[0,0])\n",
        "print('Probability(positive) =', c.predict_proba([text_to_analyse])[0,1])\n",
        "print(\"-----------------------------------------\")\n",
        "exp = explainer.explain_instance(text_to_analyse, c.predict_proba, num_features=10)\n",
        "fig = exp.as_pyplot_figure()"
      ],
      "metadata": {
        "id": "YfNAxvDvmXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Models with BOW"
      ],
      "metadata": {
        "id": "2xLjkpPezLKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install Gensim and read encoder database"
      ],
      "metadata": {
        "id": "Shu2rGY-iHuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BOW Word 2 Vec\n",
        "# https://drive.google.com/file/d/147B2WaKzmyp2m-cOtRMzYTl_Mmkj4fQ5/view?usp=sharing\n",
        "#downloaded = drive.CreateFile({'id':\"147B2WaKzmyp2m-cOtRMzYTl_Mmkj4fQ5\"})   \n",
        "#downloaded.GetContentFile('word2vec_CBOW_100d', mimetype=\"application/octet-stream\")  "
      ],
      "metadata": {
        "id": "WlNWgz8dbnOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive to Collaboratory\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "rh3Q2HBAb4mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'gdrive/MyDrive/perso/MILA/UdeM/data_competition2/word2vec_CBOW_100d'\n",
        "#filename = 'gdrive/MyDrive/perso/MILA/UdeM/data_competition2/word2vec_SkipGram_100d'"
      ],
      "metadata": {
        "id": "sTQASePUb5cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim gensim==4.2.0"
      ],
      "metadata": {
        "id": "MpDZji_LdIS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec_CBOW_100d = Word2Vec.load(filename)"
      ],
      "metadata": {
        "id": "QZB9_w9mzUc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encode dataset and create train and validation dataset"
      ],
      "metadata": {
        "id": "Oy1YsCU4iVvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to apply the word2vec encodinng to each word\n",
        "def applyWord2VecCBOW_AVG(listWords):\n",
        "    newList = []\n",
        "    for i in listWords:\n",
        "        if i in word2vec_CBOW_100d.wv:\n",
        "            newList.append(word2vec_CBOW_100d.wv[i])\n",
        "        else:\n",
        "            newList.append(np.zeros(100, dtype=float)) #prendre en compte l'absence du mot dans la moyenne\n",
        "            # voir : https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381\n",
        "    return np.mean(newList,axis=0)#np.array(newList)"
      ],
      "metadata": {
        "id": "ib0UiGtGieec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df):\n",
        "    sentences = df.copy()\n",
        "    # Converting all the upper case to lower case to avoid the distinction between them\n",
        "    sentences['text'] = df['text'].str.lower()\n",
        "    # Putting the regex for removing the https and www URLs\n",
        "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "\n",
        "    # Remove the video and links\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'{link}', '', x))\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
        "\n",
        "    # Remove html reference characters\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "\n",
        "    # Remove usernames\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'@[^\\s]+', '', x))\n",
        "\n",
        "    # Removing numbers\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # Removing hashmarks, non-letter characters\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
        "\n",
        "    # Removing all extra same letters to a limit of 2, ex. daaaang => daang, nooooooo => noo\n",
        "    #sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"(.)\\1+\", r\"\\1\\1\", x))\n",
        "        \n",
        "    return sentences"
      ],
      "metadata": {
        "id": "7hZ5hWezi6lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the sentences \n",
        "import re   \n",
        "\n",
        "train_proc = preprocessing(df_train)\n",
        "test_proc = preprocessing(df_test)\n",
        "train_proc['text'] = train_proc['text'].apply(lambda x: x.split(' '))\n",
        "test_proc['text'] = test_proc['text'].apply(lambda x: x.split(' '))\n",
        "\n",
        "#Create all datasets with different encodings\n",
        "SG_Train_AVG = train_proc['text'].apply(applyWord2VecCBOW_AVG)\n",
        "SG_Test_AVG = test_proc['text'].apply(applyWord2VecCBOW_AVG)"
      ],
      "metadata": {
        "id": "ihGTFuJli7XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the data into a matrix\n",
        "SG_Train_AVG_X = pd.DataFrame(SG_Train_AVG.tolist(), index= SG_Train_AVG.index)\n",
        "SG_Test_AVG_X = pd.DataFrame(SG_Test_AVG.tolist(), index= SG_Test_AVG.index)\n",
        "\n",
        "#Make sure all the data is numerical before training\n",
        "SG_train_x = SG_Train_AVG_X.apply(pd.to_numeric)\n"
      ],
      "metadata": {
        "id": "_cHD0D49pXjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SG_train_y = df_train_result.apply(pd.to_numeric)"
      ],
      "metadata": {
        "id": "ztB-JXE-o0-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SG_test_x = SG_Test_AVG_X.apply(pd.to_numeric)"
      ],
      "metadata": {
        "id": "J5PpZyX_lchV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = SG_train_x.to_numpy()\n",
        "train_inputs = train[:]\n",
        "train_results = SG_train_y.to_numpy()\n",
        "train_labels = train_results[:,1]\n",
        "\n",
        "test = SG_test_x.to_numpy()\n",
        "test_inputs = test[:]"
      ],
      "metadata": {
        "id": "caSGNbEWj_sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train_inputs, train_labels, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "zcckMAE-lK44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM"
      ],
      "metadata": {
        "id": "BU8iQGdGm6Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = dict(enumerate(dist_train_labels, 0))\n",
        "cls = SGDClassifier(max_iter=100, class_weight=class_weight, \n",
        "                    alpha=0.0001, penalty='l2', early_stopping=False, validation_fraction=0.1)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "\n",
        "print(cls.score(X_test, y_test)) #75.37% best"
      ],
      "metadata": {
        "id": "PonAFaoXm9EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_grid_search = False\n",
        "\n",
        "if mode_grid_search == True:\n",
        "# Find best C and Regularisation for SVM\n",
        "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "    from scipy.stats import uniform, randint\n",
        "\n",
        "    # source : https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn/notebook\n",
        "    def report_best_scores(results, n_top=3):\n",
        "        for i in range(1, n_top + 1):\n",
        "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "            for candidate in candidates:\n",
        "                print(\"Model with rank: {0}\".format(i))\n",
        "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                      results['mean_test_score'][candidate],\n",
        "                      results['std_test_score'][candidate]))\n",
        "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "                print(\"\")\n",
        "\n",
        "    SVM_model = SGDClassifier(max_iter=100, class_weight=class_weight, \n",
        "                      early_stopping=False, validation_fraction=0.1)\n",
        "\n",
        "    params = {\n",
        "        \"alpha\": [1, 0.1, 1e-3, 1e-5, 1e-6, 1e-7], #uniform(1, 1e-6),\n",
        "        \"penalty\": ['l2', 'l1']\n",
        "    }\n",
        "\n",
        "    search = GridSearchCV(SVM_model, param_grid=params,\n",
        "                        verbose=1, n_jobs=1, return_train_score=True)\n",
        "\n",
        "\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    report_best_scores(search.cv_results_, 1)"
      ],
      "metadata": {
        "id": "Cly3k8mnwPLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NB"
      ],
      "metadata": {
        "id": "RytQbqwY2LAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_nn = X_train.copy()\n",
        "scaler.fit(X_train_nn)\n",
        "X_train_nn = scaler.transform(X_train_nn)\n",
        "\n",
        "X_test_nn = X_test.copy()\n",
        "scaler.fit(X_test_nn)\n",
        "X_test_nn = scaler.transform(X_test_nn)"
      ],
      "metadata": {
        "id": "ZxXuH7Qg9yvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls = MultinomialNB(alpha=1,class_prior=dist_train_labels)\n",
        "\n",
        "cls.fit(X_train_nn, y_train)\n",
        "\n",
        "print(cls.score(X_train_nn, y_train))\n",
        "\n",
        "print(cls.score(X_test_nn, y_test))"
      ],
      "metadata": {
        "id": "2WDASgFx2P-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM"
      ],
      "metadata": {
        "id": "brm6pEOTsi5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SpatialDropout1D, Embedding"
      ],
      "metadata": {
        "id": "CY89HfXnsk2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 being negative, 1 being neutral and 2 being positive class.\n",
        "# ---> 0 being negative, 1 being positive\n",
        "y_train_2class = y_train\n",
        "y_train_2class[y_train_2class==2] = 1"
      ],
      "metadata": {
        "id": "L8iovPB54aKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train_2class))"
      ],
      "metadata": {
        "id": "Ya1kxt0G4TS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_out = 100\n",
        "max_features = 100\n",
        "embed_dim = 100\n",
        "input_length = X_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, embed_dim,input_length =input_length))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "\n",
        "model.add(LSTM(lstm_out, dropout=0.2))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "zSX_KoN6sn5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "seqModel = model.fit(X_train, y_train_2class, epochs = num_epochs, batch_size=batch_size, verbose = 2, \n",
        "          shuffle=False,use_multiprocessing=True,workers=4,validation_split=0.1)"
      ],
      "metadata": {
        "id": "lWLKsN4k-ieq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# visualizing losses and accuracy\n",
        "train_loss = seqModel.history['loss']\n",
        "val_loss   = seqModel.history['val_loss']\n",
        "train_acc  = seqModel.history['accuracy']\n",
        "val_acc    = seqModel.history['val_accuracy']\n",
        "xc         = range(num_epochs)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(xc, train_loss)\n",
        "plt.plot(xc, val_loss)"
      ],
      "metadata": {
        "id": "GYIZHX25C7ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 being negative, 1 being neutral and 2 being positive class.\n",
        "# ---> 0 being negative, 1 being positive\n",
        "y_test_2class = y_test\n",
        "y_test_2class[y_test_2class==2] = 1"
      ],
      "metadata": {
        "id": "cW0BEI5C_Zku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.evaluate(X_test, y_test_2class))"
      ],
      "metadata": {
        "id": "hDmDGkze_GLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'gdrive/MyDrive/perso/MILA/UdeM/data_competition2/neural_network_lstm_new2.h5'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "3R2pJIigA1P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest"
      ],
      "metadata": {
        "id": "VNm6vCgER22Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf_model = rf.fit(X_train, y_train)\n",
        "\n",
        "print(cls.score(X_train, y_train))\n",
        "\n",
        "print(cls.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "NUWBApbzR42C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CNN"
      ],
      "metadata": {
        "id": "RLP4HfPQ1dnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.memmap import ndarray\n",
        "#Functions to apply the word2vec encodinng to each word\n",
        "def applyWord2VecCBOW_Full(listWords, nb_rows=20):\n",
        "    newList = []\n",
        "    j = nb_rows\n",
        "    for i in listWords:\n",
        "        if j==0:\n",
        "          break\n",
        "        if i in word2vec_CBOW_100d.wv:\n",
        "            if len(newList) == 0:\n",
        "              newList = np.array([word2vec_CBOW_100d.wv[i]])\n",
        "            else:\n",
        "              newList = np.vstack([newList, word2vec_CBOW_100d.wv[i]])\n",
        "            j = j - 1\n",
        "            #newList.append(word2vec_CBOW_100d.wv[i])\n",
        "        #else:\n",
        "\n",
        "            #newList.append(np.zeros(100, dtype=float)) #prendre en compte l'absence du mot dans la moyenne\n",
        "            # voir : https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381\n",
        "    for i in range(max(0,nb_rows-len(newList))):\n",
        "      newList = np.vstack([newList, np.zeros(100, dtype=float)])\n",
        "      #newList.append(np.zeros(100, dtype=float)) \n",
        "    return newList#.astype(ndarray)#np.mean(newList,axis=0)"
      ],
      "metadata": {
        "id": "IS-SEqf21gb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the sentences \n",
        "import re   \n",
        "\n",
        "train_proc = preprocessing(df_train)\n",
        "train_proc['text'] = train_proc['text'].apply(lambda x: x.split(' '))"
      ],
      "metadata": {
        "id": "Ne-3kX6y1wry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = applyWord2VecCBOW_Full([\"computer\",\"computer\",\"computer\"],4)"
      ],
      "metadata": {
        "id": "Twuwx3Kjhktn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(test)"
      ],
      "metadata": {
        "id": "aIE6pIGWh1jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "#Create all datasets with different encodings\n",
        "SG_Train_AVG = train_proc['text'].apply(applyWord2VecCBOW_Full)\n",
        "del train_proc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "C_PqMTU1gtyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_full = SG_Train_AVG.to_numpy()\n",
        "del SG_Train_AVG\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "_eZVsmBg2f6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_full[5].shape"
      ],
      "metadata": {
        "id": "EJjFKSLmZF8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test = X_train_full[0]\n",
        "#test = np.pad(test, ((0,100-test.shape[0]),(0,0)), 'constant')\n",
        "#print(test.shape)"
      ],
      "metadata": {
        "id": "vYaP6BxD_xD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_full_pad = [np.pad(o, ((0,100-o.shape[0]),(0,0)), 'constant') for o in X_train_full]"
      ],
      "metadata": {
        "id": "uh4kVLB_Cvci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_full_pad.shape"
      ],
      "metadata": {
        "id": "eWPvxdnXC-7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion from categorical labels to numeric labels\n",
        "df_train_result[df_train_result[\"target\"]=='negative']=0\n",
        "df_train_result[df_train_result[\"target\"]=='neutral']=1\n",
        "df_train_result[df_train_result[\"target\"]=='positive']=1\n",
        "\n",
        "Y_train_full = (df_train_result.apply(pd.to_numeric)).to_numpy()\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    train_inputs, train_labels, test_size=0.1, random_state=0)"
      ],
      "metadata": {
        "id": "7svPnwpi2QzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_full = Y_train_full[:,1]"
      ],
      "metadata": {
        "id": "NGIUhHOVa0ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.histogram(Y_train_full,2)"
      ],
      "metadata": {
        "id": "Anha9ogWbZeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, Y_train_full, test_size=1/10, random_state=0)"
      ],
      "metadata": {
        "id": "q2Fuj9ZCeSOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class matrices\n",
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train, 2)\n",
        "y_val = np_utils.to_categorical(y_val, 2)"
      ],
      "metadata": {
        "id": "QrkmAWxGeuXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reshaped = [obs.reshape(1, 20, 100, 1) for obs in X_train]\n",
        "#X_train = X_train.reshape(X_train.shape[0], 20, 100, 1)\n",
        "#X_val = X_val.reshape(X_val.shape[0], 20, 100, 1)"
      ],
      "metadata": {
        "id": "WB9x1m9cfMvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reshaped = np.array(X_train_reshaped)"
      ],
      "metadata": {
        "id": "jApt35BFs7hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "L78-e7Yij1Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epoch_relu =  35 #init: 10, best:35\n",
        "learning_rate = 1e-3 #init 1e-1, best:1e-3\n",
        "batch_size = 100 \n",
        "\n",
        "validation_split=0"
      ],
      "metadata": {
        "id": "8M64sLfHj2KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32,kernel_size=(5, 5),activation='relu',input_shape=(26, 100, 1),padding='same'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization()) #init: rien\n",
        "model1.add(Dropout(0.4)) #init: rien\n",
        "model1.add(Conv2D(64,kernel_size=(5, 5),activation='relu',input_shape=(26, 100, 1),padding='same'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization()) #init: rien\n",
        "model1.add(Dropout(0.4)) #init: rien\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128,  input_dim=model1.output_shape[1])) #init:100, best:128\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(BatchNormalization()) #init: rien\n",
        "model1.add(Dropout(0.4)) #init: rien\n",
        "model1.add(Dense(2,  input_dim=128)) #init:100, best:128\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "sgd = keras.optimizers.Adam(learning_rate) #init: .SGD #best: .Adam\n",
        "model1.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "70bW8Bk3kCzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = [obs.reshape(1, 20, 100, 1) for obs in X_train]\n",
        "print(t.shape)"
      ],
      "metadata": {
        "id": "eEyUDUrKlJXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[500].shape"
      ],
      "metadata": {
        "id": "FuiAjFCTmZfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "historic = model1.fit(X_train, y_train,batch_size=batch_size, \n",
        "          epochs=nb_epoch_relu,verbose=0)\n",
        "\n",
        "end = time.time()\n",
        "convnet_relu_elasped_time = end - start\n",
        "print(\"The time used to execute : \", convnet_relu_elasped_time)"
      ],
      "metadata": {
        "id": "UJ6aMffskiUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_proc = preprocessing(df_test)\n",
        "#test_proc['text'] = test_proc['text'].apply(lambda x: x.split(' '))\n",
        "#SG_Test_AVG = test_proc['text'].apply(applyWord2VecCBOW_Full)\n",
        "#del test_proc\n",
        "#gc.collect()\n",
        "#X_test_full = SG_Test_AVG.to_numpy()\n",
        "#del SG_Test_AVG\n",
        "#gc.collect()"
      ],
      "metadata": {
        "id": "ngIuHSq4YJen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reclassification of near 50% pos or neg to neutral"
      ],
      "metadata": {
        "id": "3GV_CnAUy5Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For exploration purpose only\n",
        "pred_proba = cls.predict_proba(X_train)\n",
        "\n",
        "indice_possible_neutre = np.abs(np.round(pred_proba[:,0]-0.5,2)).astype(float) <= 0.001\n",
        "print(np.sum(indice_possible_neutre))\n",
        "possible_neutre = y_train[indice_possible_neutre]\n",
        "count_neg = len(possible_neutre[possible_neutre==0])\n",
        "count_pos = len(possible_neutre[possible_neutre==2])\n",
        "print(possible_neutre)\n",
        "print(count_neg/len(y_train), count_pos/len(y_train))\n",
        "y_train_neutral = np.copy(y_train)\n",
        "y_train_neutral[indice_possible_neutre] = 2\n",
        "\n",
        "dist_y_train_neutral = [np.sum(y_train_neutral==0)/len(y_train_neutral),\n",
        "  np.sum(y_train_neutral==2)/len(y_train_neutral)]\n",
        "\n",
        "print(dist_y_train_neutral)\n",
        "\n",
        "cls = MultinomialNB(alpha=1,class_prior=dist_y_train_neutral) #Naive Bayes\n",
        "X_train = vectorizer.transform(X_train_text)\n",
        "cls.fit(X_train, y_train_neutral)\n",
        "\n",
        "cls.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "tMzZhiq_LV3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat GPT"
      ],
      "metadata": {
        "id": "-cNa39y7zHjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "JuBrON7NzMsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"YOUR KEY\" \n",
        "openai.organization = \"YOUR ORG\""
      ],
      "metadata": {
        "id": "YtyET_7NzQjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 210000\n",
        "print(X_train_text[i],'-->', y_train[i])"
      ],
      "metadata": {
        "id": "mhBCNdP_0Z7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = X_train_text[i]\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\\n\\nTweet: \\\"%s\\\"\\nSentiment:\"%text,\n",
        "  temperature=0,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "ezYm97qdzU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Kaggle File for last model"
      ],
      "metadata": {
        "id": "Ynyc9RBEzHiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_text = test_inputs[:,1]\n",
        "X_test_kaggle = vectorizer.transform(test_input_text)\n",
        "\n",
        "test_pred = cls.predict(X_test_kaggle)\n",
        "\n",
        "PrepareKaggleFile(test_inputs=test_inputs, test_predictions=test_pred, file='tests_label_svm_small_c.csv')"
      ],
      "metadata": {
        "id": "HOYw6sVAUogy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random prediction"
      ],
      "metadata": {
        "id": "BV-PWCC64BGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random prediction but aligned with the prior distribution of training labels\n",
        "p=dist_train_labels\n",
        "test_pred_alea = np.random.choice(list_classes, len(test_inputs),p=p)\n",
        "\n",
        "PrepareKaggleFile(test_inputs=test_inputs, test_predictions=test_pred_alea, file='tests_label_alea.csv')"
      ],
      "metadata": {
        "id": "QYgaW64639UR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}